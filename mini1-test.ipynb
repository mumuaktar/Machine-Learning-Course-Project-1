{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"proj1_data.json\") as fp:\n",
    "    data = json.load(fp)\n",
    "\n",
    "# Now the data is loaded.\n",
    "# It a list of data points, where each datapoint is a dictionary with the following attributes:\n",
    "# popularity_score : a popularity score for this comment (based on the number of upvotes) (type: float)\n",
    "# children : the number of replies to this comment (type: int)\n",
    "# text : the text of this comment (type: string)\n",
    "# controversiality : a score for how \"controversial\" this comment is (automatically computed by Reddit)\n",
    "# is_root : if True, then this comment is a direct reply to a post; if False, this is a direct reply to another comment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text : \"It's days like this that make me wish I'd listened to what my mother always told me when I was a kid.\"\n",
      "\n",
      "\"Why, what did she say?\"\n",
      "\n",
      "\"I don't know. I didn't listen.\"\n",
      "is_root : False\n",
      "controversiality : 0\n",
      "children : 0\n",
      "popularity_score : 1.7198436079795936\n"
     ]
    }
   ],
   "source": [
    "# Example:\n",
    "data_point = data[1009] # select the first data point in the dataset\n",
    "\n",
    "# Now we print all the information about this datapoint\n",
    "for name, value in data_point.items():\n",
    "    print(name + \" : \" + str(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training data points: 10000\n",
      "Number of evaluation data points: 1000\n",
      "Number of test data points: 1000\n"
     ]
    }
   ],
   "source": [
    "# Splitting data into partitions\n",
    "\n",
    "no_data_point = 12000\n",
    "training_data_points = 10000\n",
    "validation_data_points = 1000\n",
    "test_data_points = 1000\n",
    "\n",
    "train_data = data[ : training_data_points]\n",
    "validation_data = data[training_data_points : (training_data_points + validation_data_points)]\n",
    "test_data = data[(training_data_points + validation_data_points) : no_data_point]\n",
    "\n",
    "print(\"Number of training data points: \" + str(len(train_data)))\n",
    "print(\"Number of evaluation data points: \" + str(len(validation_data)))\n",
    "print(\"Number of test data points: \" + str(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight vector initialization\n",
    "def weights_init(m, key):\n",
    "    # key = True => zero initialization\n",
    "    # key = False => Random initialization\n",
    "    if key:\n",
    "        w = np.zeros((m,1))\n",
    "    else:\n",
    "        w = np.random.randi(m,1)\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text pre-processing \n",
    "\n",
    "def text_prep(text):\n",
    "    punctuations_list = '!-[]{};\\,<>/\"?#$%^&*_~+' + '\\n\\n'\n",
    "    output_text = \"\"\n",
    "    text = text.strip()\n",
    "    \n",
    "    for ch in text:\n",
    "        if ch not in punctuations_list:\n",
    "            output_text = output_text + ch\n",
    "    \n",
    "    output_text = output_text.lower()\n",
    "    prep_text = output_text.split(\" \")\n",
    "    prep_text = list(filter(None,prep_text))\n",
    "    \n",
    "    return prep_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most-frequent word count features for the whole Training set\n",
    "def Feature_Matrix(dataset, no_txt_features):\n",
    "    \n",
    "    offensive_words_list=['shit','fuck','fucking','bitch','damn','sex','ass','hell',\n",
    "         'hot','dick','shitty','fucked','asshole','bullshit','gay','porn','crap','sucks']\n",
    "    \n",
    "    positive_sentiments_list = ['like', 'really', 'good', 'please', 'love', 'pretty', 'best', \n",
    "                                'better', 'great', 'movie', 'happy', 'watching', 'nice', 'fun', \n",
    "                                'thanks', 'thank', 'funny', 'cool', 'thankfully', 'super', 'enjoy', \n",
    "                                'awesome' , 'wow', 'amazing', 'interesting', 'loved’, ‘liked', 'perfect',\n",
    "                                'fan', 'glad', 'haha', 'fans', 'hilarious', 'popular', 'fair', 'special', \n",
    "                                'beautiful', ':d', ':)', '=d']\n",
    "    \n",
    "    negative_sentiments_list = ['bad', 'hate', 'wrong', 'lost', 'damn', 'hell', 'sorry', 'dead', 'weird', 'shitty',\n",
    "                                'worst', 'terrible', 'worse', 'sad', 'seeing', 'die', 'death', 'died', 'kill', 'poor',\n",
    "                                'breaking', 'horrible', ':(', '=(']\n",
    "    \n",
    "    N = len(dataset)\n",
    "    All_comments = [None] * N\n",
    "    Words_Dict = {}\n",
    "    bias = np.ones((N,1))\n",
    "    controversiality_vec = np.zeros((N,1))\n",
    "    is_root_vec = np.zeros((N,1))\n",
    "    children_vec = np.zeros((N,1))\n",
    "    Y = np.zeros((N,1))\n",
    "    X_words_count = np.zeros((N,no_txt_features))\n",
    "    offensive_count = np.zeros((N,1))\n",
    "    http_count = np.zeros((N,1))\n",
    "    positive_sentiments = np.zeros((N,1))\n",
    "    negative_sentiments = np.zeros((N,1))\n",
    "    \n",
    "    for i in range(N):\n",
    "        for key,val in dataset[i].items():\n",
    "            if key == 'text':\n",
    "                preprocessed_text = text_prep(val)\n",
    "                All_comments[i] = preprocessed_text\n",
    "                for x in preprocessed_text:\n",
    "                    if x not in Words_Dict.keys():\n",
    "                        Words_Dict[x] = 0\n",
    "                    Words_Dict[x] += 1\n",
    "            \n",
    "            elif key == 'is_root':\n",
    "                if val:\n",
    "                    is_root_vec[i] = 1\n",
    "                else:\n",
    "                    is_root_vec[i] = 0\n",
    "\n",
    "            elif key == 'controversiality':\n",
    "                controversiality_vec[i] = val\n",
    "            \n",
    "            elif key == 'children':\n",
    "                children_vec[i] = val\n",
    "            \n",
    "            elif key == 'popularity_score':\n",
    "                Y[i] = val\n",
    "\n",
    "# Now we need to sort out from most-frequent to least-frequent words in dictionary to obtain the first N words\n",
    "    if no_txt_features == 0 :\n",
    "        \n",
    "        X1 = np.append(children_vec,controversiality_vec,axis=1)\n",
    "        X1 = np.append(X1,is_root_vec,axis=1)        \n",
    "        X = np.append(X1,bias,axis=1)\n",
    "        \n",
    "    else:\n",
    "        Words_Dict_Sorted = sorted(Words_Dict.items(), key = lambda t: t[1], reverse=True)\n",
    "        Most_Freq_Words_Dict = dict(list(Words_Dict_Sorted[:no_txt_features]))\n",
    "        Most_Freq_Words = list(Most_Freq_Words_Dict.keys())\n",
    "        \n",
    "# Now we need to count the frequency of most frequent words in each comment throughout the whole dataset\n",
    "                \n",
    "        # Most-Frequent words\n",
    "        for i in range(N):\n",
    "            for j in range(no_txt_features):  \n",
    "                X_words_count[i,j] = All_comments[i].count(Most_Freq_Words[j])     \n",
    "        \n",
    "        # Offensive and HTTP-containing comments\n",
    "        \n",
    "        for i in range(N):\n",
    "            for x in All_comments[i]:\n",
    "                if x.find('http') or x.find('www.') != -1 :\n",
    "                    http_count[i] = 1\n",
    "                \n",
    "                for l in range(len(positive_sentiments_list)):\n",
    "                    if x.find(positive_sentiments_list[l]) != -1 :\n",
    "                        positive_sentiments[i] += 1\n",
    "                \n",
    "                for l in range(len(negative_sentiments_list)):\n",
    "                    if x.find(negative_sentiments_list[l]) != -1 :\n",
    "                        negative_sentiments[i] += 1\n",
    "                        \n",
    "            for j in range(len(offensive_words_list)):  \n",
    "                offensive_count[i] += All_comments[i].count(offensive_words_list[j])\n",
    "            \n",
    "        for j in range(len(offensive_words_list)):  \n",
    "            if offensive_count[i] > 0:\n",
    "                offensive_count[i] = 1\n",
    "        \n",
    "        \n",
    "        # Positive or negative sentiments\n",
    "        \n",
    "        X1 = np.append(X_words_count,controversiality_vec,axis=1)\n",
    "        X1 = np.append(X1,is_root_vec,axis=1)\n",
    "        X1 = np.append(X1,children_vec,axis=1)\n",
    "        X1 = np.append(X1,offensive_count,axis=1)\n",
    "        X1 = np.append(X1,http_count,axis=1)\n",
    "        X1 = np.append(X1,positive_sentiments,axis=1)\n",
    "        X1 = np.append(X1,negative_sentiments,axis=1)\n",
    "        \n",
    "        # Data Rescaling\n",
    "        \n",
    "        for i in range(X1.shape[1]):\n",
    "            mean = np.mean(X1[:,i])\n",
    "            var = np.var(X1[:,i])\n",
    "            X1[:,i] = (1 / np.sqrt(var)) * (X1[:,i] - mean)\n",
    "        \n",
    "        X = np.append(X1,bias,axis=1)       \n",
    "        \n",
    "    return X, Y, Most_Freq_Words_Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Least-Square estimation\n",
    "\n",
    "def Least_Squares_Estimation(X,Y):\n",
    "    X_T = (X.T).dot(X)\n",
    "    X_T_inv = np.linalg.inv(X_T)\n",
    "    X_Y = (X.T).dot(Y)\n",
    "    w_hat = np.dot(X_T_inv, X_Y)\n",
    "\n",
    "    return w_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent\n",
    "def Gradient_Descent(X, Y, W0, B, E, eps):\n",
    "    \n",
    "    eta0 = E\n",
    "    beta = B\n",
    "    epsilon = eps\n",
    "    w0 = W0\n",
    "    mse = np.zeros(100000000) # some big number for number of epochs\n",
    "    X_T = np.dot(X.T, X)\n",
    "    X_Y = np.dot(X.T, Y)\n",
    "    alpha = eta0 / ( 1 + beta )\n",
    "    w_gd = w0 - 2 * alpha * (np.subtract(np.dot(X_T,w0), X_Y))\n",
    "    diff = np.linalg.norm(np.subtract(w_gd,w0))\n",
    "    epoch = 0\n",
    "    mse[epoch] = Mean_Square_Error(X, Y, w_gd)\n",
    "    \n",
    "    while diff > epsilon:\n",
    "        w0 = w_gd\n",
    "        alpha =  eta0 / (1 + beta * (epoch + 1))\n",
    "        w_gd = w0 - 2 * alpha * np.subtract((X_T).dot(w0), X_Y)\n",
    "        diff = np.linalg.norm(np.subtract(w_gd, w0))\n",
    "        epoch +=1\n",
    "        mse[epoch] = Mean_Square_Error(X, Y, w_gd)\n",
    "    \n",
    "    MSE = np.delete(mse, np.s_[epoch + 1 : len(mse) + 1]) # Removing zero-valued MSE at the end\n",
    "    \n",
    "    return w_gd, MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE\n",
    "def Mean_Square_Error(X, Y, w_hat):\n",
    "    prediction = np.dot(X,w_hat)\n",
    "    abs_err = np.subtract(Y,prediction)\n",
    "    squared_err = np.square(abs_err)\n",
    "    MSE = (1/X.shape[0]) * np.sum(squared_err)  \n",
    "    return MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisations\n",
    "def lineplot(x_data, y_data, x_label=\"\", y_label=\"\", title=\"\", gcolor=\"\"):\n",
    "    # Create the plot object    \n",
    "    \n",
    "    # Plot the best fit line, set the linewidth (lw), color and\n",
    "    # transparency (alpha) of the line\n",
    "    plt.plot(x_data, y_data, lw = 2, color = gcolor, alpha = 1)\n",
    "    \n",
    "    # Label the axes and provide a title\n",
    "    plt.title(title)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for LS on Training set is: 1.0452516646873893 which took: 13.970136642456055 ms\n",
      "Final MSE for GD on Training set at 4402's epoch is :1.0452517672030792 which took: 2437.9935264587402 ms\n",
      "MSE for LS on Validation set is: 1.036421488915833\n",
      "Final MSE for GD on Validation set is :1.0363143038778946\n"
     ]
    }
   ],
   "source": [
    "# Performance Evaluation\n",
    "no_text_features = 160\n",
    "Eta0 = 1e-6\n",
    "Beta0 = 0.0001\n",
    "epsilon = 1e-6\n",
    "X,Y,most_freq_words = Feature_Matrix(train_data, no_text_features)\n",
    "W0 = weights_init(X.shape[1], True) # All_zero weights initialization if True\n",
    "\n",
    "start1 = time.time()\n",
    "W_LS = Least_Squares_Estimation(X, Y)\n",
    "MSE_LS_training = Mean_Square_Error(X, Y, W_LS)\n",
    "end1 = time.time()\n",
    "\n",
    "print(\"MSE for LS on Training set is: \" + str(MSE_LS_training) + \" which took: \" + str((end1-start1) * 1000) + \" ms\" + \"\")\n",
    "\n",
    "start2 = time.time()\n",
    "W_GD, MSE_GD_training = Gradient_Descent(X, Y, W0, Beta0, Eta0, epsilon)\n",
    "end2 = time.time()\n",
    "print(\"Final MSE for GD on Training set at \" + str(len(MSE_GD_training)) + \"'s epoch is :\" + str(MSE_GD_training[-1]) +\n",
    "     \" which took: \" + str((end2-start2) * 1000) + \" ms\" + \"\")\n",
    "\n",
    "\n",
    "# Run on Validation dataset\n",
    "\n",
    "X_validation, Y_validation, _ = Feature_Matrix(validation_data, no_text_features)\n",
    "\n",
    "MSE_LS_validation = Mean_Square_Error(X_validation, Y_validation, W_LS)\n",
    "\n",
    "print(\"MSE for LS on Validation set is: \" + str(MSE_LS_validation))\n",
    "\n",
    "MSE_GD_validation = Mean_Square_Error(X_validation, Y_validation, W_GD)\n",
    "\n",
    "print(\"Final MSE for GD on Validation set is :\" + str(MSE_GD_validation))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 9250, 'i': 6132, 'a': 5914, 'to': 5880, 'and': 5664, 'of': 4101, 'that': 3073, 'you': 3050, 'it': 3035, 'in': 2922, 'is': 2380, 'was': 2377, 'my': 2264, 'for': 2019, 'have': 1675, 'but': 1635, 'this': 1550, 'with': 1485, 'on': 1418, 'not': 1361, 'be': 1251, 'just': 1215, 'or': 1195, 'so': 1165, 'they': 1160, 'like': 1151, 'if': 1137, 'your': 1135, 'me': 1112, 'are': 1051, 'at': 1020, 'as': 963, 'he': 955, \"it's\": 864, 'about': 839, 'she': 831, \"i'm\": 823, 'all': 819, 'when': 802, 'we': 794, 'out': 794, 'her': 784, 'because': 771, 'from': 750, 'would': 738, 'get': 731, \"don't\": 725, 'up': 723, 'what': 719, 'one': 713, 'had': 703, 'people': 697, 'do': 668, 'can': 666, 'an': 643, 'deleted': 626, 'there': 606, 'no': 544, 'some': 536, 'really': 533, 'any': 524, 'how': 511, 'his': 497, 'then': 495, 'think': 477, 'more': 470, 'them': 465, 'time': 455, 'go': 453, 'their': 449, 'were': 444, 'who': 440, 'will': 435, 'know': 427, 'by': 413, 'has': 407, 'only': 397, 'good': 390, 'new': 382, \"you're\": 381, 'been': 380, 'got': 379, 'it.': 379, 'even': 378, 'after': 372, 'him': 368, 'make': 368, 'never': 363, 'please': 354, 'am': 349, 'did': 349, 'still': 348, 'something': 346, 'than': 346, \"that's\": 334, 'much': 330, 'being': 330, 'into': 326, 'other': 320, 'going': 316, \"didn't\": 316, 'way': 315, 'post': 310, 'first': 309, 'could': 306, 'removed': 303, 'also': 301, 'now': 280, 'want': 280, 'say': 280, 'where': 278, 'over': 277, 'why': 275, \"can't\": 265, 'see': 265, 'most': 264, 'back': 260, 'too': 260, 'shit': 258, 'season': 252, 'show': 252, 'pretty': 251, 'friends': 250, 'love': 247, 'very': 247, 'always': 247, 'life': 244, 'made': 243, 'questions': 242, 'years': 241, 'before': 239, 'our': 238, \"i've\": 238, 'contact': 235, 'off': 235, 'every': 234, 'which': 234, 'right': 232, 'fuck': 227, 'actually': 226, 'well': 224, 'fucking': 223, 'moderators': 222, 'things': 222, 'thing': 219, 'here': 216, 'lot': 214, 'question': 214, 'feel': 210, 'same': 209, 'its': 207, 'said': 207, 'bad': 204, 'use': 204, 'should': 203, 'work': 201, 'action': 200, 'while': 200, 'few': 200, \"doesn't\": 199}\n"
     ]
    }
   ],
   "source": [
    "print(most_freq_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text features: 0\n",
      "MSE for LS on Training set is: 1.0846830709157251 which took: 0.0 ms\n",
      "Final MSE for GD on Training set at 43323's epoch is :1.0846892312314529 which took: 2132.9681873321533 ms\n",
      " \n",
      "MSE for LS on Validation set is: 1.020326684843145\n",
      "Final MSE for GD on Validation set is :1.0204268695519987\n",
      " \n",
      "Number of text features: 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-235-3158ee9f925d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mno_text_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumber_text_features\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Number of text features: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mno_text_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFeature_Matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mno_text_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mW0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweights_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# All_zero weights initialization if True\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-230-474efc376249>\u001b[0m in \u001b[0;36mFeature_Matrix\u001b[1;34m(dataset, no_txt_features)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpositive_sentiments_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m                     \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpositive_sentiments_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m                         \u001b[0mpositive_sentiments\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Performance Evaluation for a range of number_of_features in order to identify best model\n",
    "\n",
    "number_text_features = np.arange(0,161)\n",
    "Eta0 = 1e-6\n",
    "Beta0 = 0.0001\n",
    "epsilon = 1e-6\n",
    "result_train = np.zeros((len(number_text_features),2))\n",
    "result_valid = np.zeros((len(number_text_features),2))\n",
    "\n",
    "\n",
    "for i in range(len(number_text_features)):\n",
    "    no_text_features = number_text_features[i]\n",
    "    print(\"Number of text features: \" + str(no_text_features))\n",
    "    X,Y,_ = Feature_Matrix(train_data, no_text_features)\n",
    "    W0 = weights_init(X.shape[1], True) # All_zero weights initialization if True\n",
    "\n",
    "    start1 = time.time()\n",
    "    W_LS = Least_Squares_Estimation(X, Y)\n",
    "    MSE_LS_training = Mean_Square_Error(X, Y, W_LS)\n",
    "    end1 = time.time()\n",
    "    print(\"MSE for LS on Training set is: \" + str(MSE_LS_training) + \" which took: \" + str((end1-start1) * 1000) + \" ms\" + \"\")\n",
    "\n",
    "    start2 = time.time()\n",
    "    W_GD, MSE_GD_training = Gradient_Descent(X, Y, W0, Beta0, Eta0, epsilon)\n",
    "    end2 = time.time()\n",
    "    print(\"Final MSE for GD on Training set at \" + str(len(MSE_GD_training)) + \"'s epoch is :\" + str(MSE_GD_training[-1]) +\n",
    "         \" which took: \" + str((end2-start2) * 1000) + \" ms\" + \"\")\n",
    "    print(\" \")\n",
    "    result_train[i,0] = MSE_LS_training\n",
    "    result_train[i,1] = MSE_GD_training[-1]\n",
    "\n",
    "    # Run on Validation dataset\n",
    "    X_validation, Y_validation,_ = Feature_Matrix(validation_data, no_text_features)\n",
    "\n",
    "    MSE_LS_validation = Mean_Square_Error(X_validation, Y_validation, W_LS)\n",
    "\n",
    "    print(\"MSE for LS on Validation set is: \" + str(MSE_LS_validation))\n",
    "\n",
    "    MSE_GD_validation = Mean_Square_Error(X_validation, Y_validation, W_GD)\n",
    "\n",
    "    print(\"Final MSE for GD on Validation set is :\" + str(MSE_GD_validation))\n",
    "    print(\" \")\n",
    "\n",
    "    result_valid[i,0] = MSE_LS_validation\n",
    "    result_valid[i,1] = MSE_GD_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for LS on Test set is: 1.2707377671901572\n",
      "MSE for GD on Test set is :1.2707013325581547\n"
     ]
    }
   ],
   "source": [
    "# Run on Test dataset\n",
    "\n",
    "# no_text_features = 0\n",
    "X_test, Y_test,_ = Feature_Matrix(test_data, no_text_features)\n",
    "\n",
    "MSE_LS_Test = Mean_Square_Error(X_test, Y_test, W_LS)\n",
    "\n",
    "print(\"MSE for LS on Test set is: \" + str(MSE_LS_Test))\n",
    "\n",
    "MSE_GD_validation = Mean_Square_Error(X_test, Y_test, W_GD)\n",
    "\n",
    "print(\"MSE for GD on Test set is :\" + str(MSE_GD_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "lineplot(range(MSE_GD_training.shape[0]), MSE_GD_training, \n",
    "         x_label=\"epochs\", y_label=\"MSE\", title=\"Gradient Descent for Training set\", gcolor='b')\n",
    "# lineplot(range(MSE_GD_training.shape[0]), MSE_GD_validation, \n",
    "#          x_label=\"epochs\", y_label=\"MSE\", title=\"Gradient Descent\", gcolor='r')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
